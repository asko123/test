{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Chat Bot with Knowledge Graph & ReAct Agent\n",
    "\n",
    "This notebook provides an interactive chat interface for querying multiple documents (PDF, JSON, JSONL, TXT) with Knowledge Graph enhancement and optional ReAct Agent for complex reasoning.\n",
    "\n",
    "## Quick Start Workflow\n",
    "\n",
    "1. **Run cells 1-4**: Install packages, import libraries, configure settings\n",
    "2. **Run cell 9**: Upload your documents and extract text\n",
    "3. **Run cell 15**: Initialize Knowledge Graph and chat interface\n",
    "4. **Run cell 17**: Start continuous chat - ask unlimited questions until you type `exit()`\n",
    "5. **Run cell 19** (optional): View full chat history\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Multi-format support**: PDF, JSON, JSONL, and TXT files\n",
    "- **Knowledge Graph**: Automatically extracts entities (controls, risks, assets, etc.) and relationships\n",
    "- **ReAct Agent**: Advanced multi-step reasoning for complex queries with automatic routing\n",
    "- **Continuous chat**: Ask multiple questions in one session\n",
    "- **Chat history**: View all questions and answers\n",
    "- **Enhanced responses**: Better context through entity and relationship tracking\n",
    "\n",
    "## ReAct Agent with Intelligent Routing\n",
    "\n",
    "The **ReAct Agent** uses your LLM iteratively with specialized tools for complex multi-step reasoning. The agent **automatically activates** for complex queries!\n",
    "\n",
    "### How Agent-LLM Interaction Works\n",
    "\n",
    "```\n",
    "USER QUERY: \"What would be impacted if we remove AC-2?\"\n",
    "    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ LLM CALL #1: Planning               │\n",
    "│ \"I need to find AC-2 first\"         │\n",
    "│ → Decides to use search_entities    │\n",
    "└─────────────────────────────────────┘\n",
    "    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ TOOL: search_entities(\"AC-2\")       │\n",
    "│ → Returns: CONTROL_AC-2 found       │\n",
    "└─────────────────────────────────────┘\n",
    "    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ LLM CALL #2: Interpret              │\n",
    "│ \"Found AC-2. Now get relationships\" │\n",
    "│ → Decides to use get_relationships  │\n",
    "└─────────────────────────────────────┘\n",
    "    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ TOOL: get_entity_relationships      │\n",
    "│ → Returns: Connected entities       │\n",
    "└─────────────────────────────────────┘\n",
    "    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ LLM CALL #3: Continue               │\n",
    "│ \"Need downstream impacts\"           │\n",
    "│ → Decides to use traverse_graph     │\n",
    "└─────────────────────────────────────┘\n",
    "    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ TOOL: traverse_graph(depth=2)       │\n",
    "│ → Returns: Full dependency tree     │\n",
    "└─────────────────────────────────────┘\n",
    "    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ LLM CALL #4: Synthesize             │\n",
    "│ \"I have everything. Final answer:\"  │\n",
    "│ → Generates comprehensive report    │\n",
    "└─────────────────────────────────────┘\n",
    "    ↓\n",
    "COMPREHENSIVE RESPONSE\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **Same LLM** used throughout (4-10+ calls per complex query)\n",
    "- **Agent = LLM + Tools + Iterative Reasoning**\n",
    "- Each cycle: Plan → Execute Tool → Interpret Results → Repeat\n",
    "- Final LLM call synthesizes all gathered information\n",
    "\n",
    "**Comparison:**\n",
    "- **Simple Flow:** 1 LLM call (fast, direct answer)\n",
    "- **Agent Flow:** 4-10+ LLM calls (thorough, multi-step reasoning)\n",
    "\n",
    "**How It Works in This Notebook:**\n",
    "- Configure agent settings in the configuration cell (Cell 5)\n",
    "- Agent automatically activates when query complexity exceeds threshold (60/100)\n",
    "- Simple queries use direct KG retrieval for speed\n",
    "- Complex queries use multi-step agent reasoning for thoroughness\n",
    "\n",
    "## Commands During Chat\n",
    "\n",
    "- Type your question and press Enter\n",
    "- Type `exit()` or `quit()` to end the chat session\n",
    "- Type `history` to view chat history\n",
    "- Press Ctrl+C to interrupt (then type exit() to quit properly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install goldmansachs.awm_genai -U\n",
    "%pip install python-dotenv pandas ipywidgets pdfplumber networkx matplotlib -q\n",
    "%pip install langgraph langchain langchain-core -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goldmansachs.awm_genai import LLM, LLMConfig\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "import pdfplumber\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "app_id = \"trai\"\n",
    "env = \"uat\"\n",
    "\n",
    "# Model Configuration - Choose your model\n",
    "available_models = [\"gemini-2.5-pro\", \"gemini-2.5-flash-lite\"]\n",
    "\n",
    "# Create model selection widget\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=\"gemini-2.5-flash-lite\",\n",
    "    description='Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Agent Configuration - NEW!\n",
    "enable_agent_toggle = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Enable ReAct Agent Mode',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "show_reasoning_toggle = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Show Agent Reasoning',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Display configuration\n",
    "display(HTML(\"<h3>Configuration</h3>\"))\n",
    "display(HTML(\"<p><b>gemini-2.5-pro:</b> More capable, better for complex questions<br><b>gemini-2.5-flash-lite:</b> Faster responses, good for simple queries</p>\"))\n",
    "display(model_selector)\n",
    "\n",
    "display(HTML(\"<br><h4>Agent Settings</h4>\"))\n",
    "display(HTML(\"<p>Enable agent mode for advanced multi-step reasoning on complex queries. The agent uses your LLM iteratively with specialized tools.</p>\"))\n",
    "display(enable_agent_toggle)\n",
    "display(show_reasoning_toggle)\n",
    "\n",
    "# Vespa Configuration\n",
    "display(HTML(\"<br><h4>Vespa Vector Store</h4>\"))\n",
    "display(HTML(\"<p>Enable Vespa search for additional context. Used as fallback when no documents uploaded.</p>\"))\n",
    "\n",
    "enable_vespa_toggle = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Enable Vespa Search',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "vespa_schema_input = widgets.Text(\n",
    "    value='tech_risk_ai',\n",
    "    description='Schema ID:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "vespa_env_selector = widgets.Dropdown(\n",
    "    options=['dev', 'uat', 'prod'],\n",
    "    value='uat',\n",
    "    description='Vespa Env:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "vespa_gssso_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter GSSSO token (optional)',\n",
    "    description='GSSSO Token:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "vespa_api_key_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter API key (optional)',\n",
    "    description='API Key:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(enable_vespa_toggle)\n",
    "display(vespa_schema_input)\n",
    "display(vespa_env_selector)\n",
    "display(HTML(\"<p><small><b>Authentication (Optional):</b> Add GSSSO token or API key if getting 401/500 errors</small></p>\"))\n",
    "display(vespa_gssso_input)\n",
    "display(vespa_api_key_input)\n",
    "\n",
    "# Store configuration\n",
    "temperature = 0\n",
    "log_level = \"DEBUG\"\n",
    "agent_temperature = 0.2\n",
    "agent_max_iterations = 10\n",
    "agent_complexity_threshold = 60\n",
    "\n",
    "print(f\"\\nApp ID: {app_id}\")\n",
    "print(f\"Environment: {env}\")\n",
    "print(f\"Agent Mode: {'Enabled' if enable_agent_toggle.value else 'Disabled'}\")\n",
    "print(f\"Vespa Search: {'Enabled' if enable_vespa_toggle.value else 'Disabled'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM with selected model\n",
    "model_name = model_selector.value\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    app_id=app_id,\n",
    "    env=env,\n",
    "    model_name=model_name,\n",
    "    temperature=temperature,\n",
    "    log_level=log_level,\n",
    ")\n",
    "\n",
    "llm = LLM.init(config=llm_config)\n",
    "print(f\"[SUCCESS] LLM initialized successfully with {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Documents\n",
    "\n",
    "Use the file upload widget below to select your PDF documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract PDF content with tables and JSON\n",
    "def extract_pdf_content(file_path: str, filename: str) -> str:\n",
    "    \"\"\"Extract text, tables, and JSON from PDF.\"\"\"\n",
    "    content_parts = [f\"\\n\\n{'='*80}\\nDocument: {filename}\\n{'='*80}\\n\"]\n",
    "    \n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            content_parts.append(f\"\\n[Page {page_num}]\\n\")\n",
    "            \n",
    "            # Extract tables on this page\n",
    "            tables = page.extract_tables()\n",
    "            \n",
    "            # Get bounding boxes of tables to exclude from text\n",
    "            table_bboxes = []\n",
    "            if tables:\n",
    "                for table in page.find_tables():\n",
    "                    table_bboxes.append(table.bbox)\n",
    "            \n",
    "            # Extract text excluding table areas\n",
    "            if table_bboxes:\n",
    "                text = page.filter(lambda obj: not any(\n",
    "                    obj.get('x0', 0) >= bbox[0] and obj.get('x1', 0) <= bbox[2] and\n",
    "                    obj.get('top', 0) >= bbox[1] and obj.get('bottom', 0) <= bbox[3]\n",
    "                    for bbox in table_bboxes\n",
    "                )).extract_text()\n",
    "            else:\n",
    "                text = page.extract_text()\n",
    "            \n",
    "            # Check for JSON/JSONL content in text\n",
    "            if text and text.strip():\n",
    "                json_objects = extract_json_content(text)\n",
    "                \n",
    "                if json_objects:\n",
    "                    # Add regular text (non-JSON parts)\n",
    "                    non_json_text = remove_json_from_text(text)\n",
    "                    if non_json_text.strip():\n",
    "                        content_parts.append(f\"{non_json_text}\\n\")\n",
    "                    \n",
    "                    # Add formatted JSON objects\n",
    "                    for json_idx, json_obj in enumerate(json_objects, 1):\n",
    "                        formatted_json = format_json_object(json_obj, page_num, json_idx, filename)\n",
    "                        content_parts.append(f\"\\n{formatted_json}\\n\")\n",
    "                else:\n",
    "                    # No JSON, add as regular text\n",
    "                    content_parts.append(f\"{text}\\n\")\n",
    "            \n",
    "            # Add tables with proper formatting\n",
    "            if tables:\n",
    "                for table_idx, table in enumerate(tables, 1):\n",
    "                    if table and len(table) > 0:\n",
    "                        formatted_table = format_table(table, page_num, table_idx, filename)\n",
    "                        content_parts.append(f\"\\n{formatted_table}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "def extract_json_content(text: str) -> list:\n",
    "    \"\"\"Extract JSON or JSONL objects from text.\"\"\"\n",
    "    json_objects = []\n",
    "    \n",
    "    # Try to find JSON objects\n",
    "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
    "    matches = re.finditer(json_pattern, text, re.DOTALL)\n",
    "    \n",
    "    for match in matches:\n",
    "        try:\n",
    "            json_str = match.group(0)\n",
    "            json_obj = json.loads(json_str)\n",
    "            json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Also try line-by-line for JSONL format\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('{') and line.endswith('}'):\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                if json_obj not in json_objects:\n",
    "                    json_objects.append(json_obj)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return json_objects\n",
    "\n",
    "def remove_json_from_text(text: str) -> str:\n",
    "    \"\"\"Remove JSON objects from text to get only regular text.\"\"\"\n",
    "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
    "    cleaned_text = re.sub(json_pattern, '', text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "\n",
    "def format_json_object(json_obj: dict, page_num: int, json_idx: int, filename: str) -> str:\n",
    "    \"\"\"Format JSON object for LLM understanding.\"\"\"\n",
    "    json_parts = []\n",
    "    json_parts.append(f\"--- JSON OBJECT {json_idx} (Document: {filename}, Page {page_num}) ---\")\n",
    "    \n",
    "    # Add formatted key-value pairs\n",
    "    json_parts.append(\"\\nStructured Data Fields:\")\n",
    "    for key, value in json_obj.items():\n",
    "        # Clean up the value\n",
    "        if isinstance(value, str):\n",
    "            value = ' '.join(value.split())\n",
    "        json_parts.append(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Add JSON format\n",
    "    json_parts.append(\"\\nJSON Format:\")\n",
    "    json_parts.append(json.dumps(json_obj, indent=2))\n",
    "    \n",
    "    json_parts.append(f\"--- END JSON OBJECT {json_idx} ---\\n\")\n",
    "    \n",
    "    return \"\\n\".join(json_parts)\n",
    "\n",
    "def format_table(table: list, page_num: int, table_idx: int, filename: str) -> str:\n",
    "    \"\"\"Format table with proper structure.\"\"\"\n",
    "    if not table or len(table) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean table data\n",
    "    cleaned_table = []\n",
    "    for row in table:\n",
    "        cleaned_row = [str(cell).strip() if cell is not None else \"\" for cell in row]\n",
    "        if any(cleaned_row):\n",
    "            cleaned_table.append(cleaned_row)\n",
    "    \n",
    "    if not cleaned_table:\n",
    "        return \"\"\n",
    "    \n",
    "    table_parts = []\n",
    "    table_parts.append(f\"--- TABLE {table_idx} (Document: {filename}, Page {page_num}) ---\")\n",
    "    \n",
    "    # Assume first row is header\n",
    "    headers = cleaned_table[0]\n",
    "    data_rows = cleaned_table[1:]\n",
    "    \n",
    "    # Add headers\n",
    "    table_parts.append(\"\\nColumn Headers:\")\n",
    "    table_parts.append(\" | \".join(headers))\n",
    "    table_parts.append(\"-\" * 80)\n",
    "    \n",
    "    # Add data rows\n",
    "    table_parts.append(\"\\nTable Data:\")\n",
    "    for row in data_rows:\n",
    "        table_parts.append(\" | \".join(row))\n",
    "    \n",
    "    # Add markdown format for better LLM understanding\n",
    "    table_parts.append(\"\\nMarkdown Format:\")\n",
    "    table_parts.append(\"| \" + \" | \".join(headers) + \" |\")\n",
    "    table_parts.append(\"|\" + \"|\".join([\"---\" for _ in headers]) + \"|\")\n",
    "    for row in data_rows:\n",
    "        table_parts.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "    \n",
    "    table_parts.append(f\"--- END TABLE {table_idx} ---\\n\")\n",
    "    \n",
    "    return \"\\n\".join(table_parts)\n",
    "\n",
    "# Helper functions for JSON extraction\n",
    "def extract_from_json_file(file_path: str, filename: str) -> str:\n",
    "    \"\"\"Extract and format JSON file content.\"\"\"\n",
    "    content_parts = [f\"\\n\\n{'='*80}\\nDocument: {filename}\\n{'='*80}\\n\"]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            content_parts.append(\"\\nThis file contains a list of JSON objects:\\n\")\n",
    "            for idx, obj in enumerate(data, 1):\n",
    "                formatted = format_json_object(obj, 0, idx, filename)\n",
    "                content_parts.append(f\"\\n{formatted}\\n\")\n",
    "        elif isinstance(data, dict):\n",
    "            formatted = format_json_object(data, 0, 1, filename)\n",
    "            content_parts.append(f\"\\n{formatted}\\n\")\n",
    "        else:\n",
    "            content_parts.append(f\"\\nJSON Value: {data}\\n\")\n",
    "    except Exception as e:\n",
    "        content_parts.append(f\"\\n[ERROR] Failed to parse JSON: {str(e)}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "def extract_from_jsonl_file(file_path: str, filename: str) -> str:\n",
    "    \"\"\"Extract and format JSONL file content.\"\"\"\n",
    "    content_parts = [f\"\\n\\n{'='*80}\\nDocument: {filename}\\n{'='*80}\\n\"]\n",
    "    content_parts.append(\"\\nThis file contains multiple JSON objects (one per line):\\n\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        formatted = format_json_object(obj, 0, idx, filename)\n",
    "                        content_parts.append(f\"\\n{formatted}\\n\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        content_parts.append(f\"\\n[Line {idx}] Invalid JSON: {line[:100]}...\\n\")\n",
    "    except Exception as e:\n",
    "        content_parts.append(f\"\\n[ERROR] Failed to parse JSONL: {str(e)}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "# Create file upload widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.pdf,.json,.jsonl,.txt',\n",
    "    multiple=True,\n",
    "    description='Select Files'\n",
    ")\n",
    "\n",
    "# Create process button\n",
    "process_button = widgets.Button(\n",
    "    description='Extract Text',\n",
    "    button_style='primary',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Create output widget for status messages\n",
    "output = widgets.Output()\n",
    "\n",
    "# Store extracted text globally\n",
    "extracted_text = None\n",
    "document_names = []\n",
    "\n",
    "def on_process_button_clicked(b):\n",
    "    global extracted_text, document_names\n",
    "    \n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        \n",
    "        if not upload_widget.value:\n",
    "            print(\"[WARNING] Please select PDF files first\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            all_text = []\n",
    "            document_names = []\n",
    "            \n",
    "            # Extract content from uploaded files\n",
    "            files = upload_widget.value\n",
    "            print(f\"Processing {len(files)} files...\\n\")\n",
    "            \n",
    "            temp_files = []\n",
    "            document_names = []\n",
    "            \n",
    "            # Save uploaded files temporarily\n",
    "            for file_info in files:\n",
    "                filename = file_info['name']\n",
    "                content = file_info['content']\n",
    "                document_names.append(filename)\n",
    "                \n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "                    tmp_file.write(content)\n",
    "                    temp_files.append((tmp_file.name, filename))\n",
    "            \n",
    "            # Extract content based on file type\n",
    "            for tmp_path, filename in temp_files:\n",
    "                try:\n",
    "                    file_ext = filename.split('.')[-1].lower()\n",
    "                    \n",
    "                    if file_ext == 'pdf':\n",
    "                        doc_content = extract_pdf_content(tmp_path, filename)\n",
    "                    elif file_ext == 'json':\n",
    "                        doc_content = extract_from_json_file(tmp_path, filename)\n",
    "                    elif file_ext == 'jsonl':\n",
    "                        doc_content = extract_from_jsonl_file(tmp_path, filename)\n",
    "                    elif file_ext == 'txt':\n",
    "                        # Extract from text file\n",
    "                        with open(tmp_path, 'r', encoding='utf-8') as f:\n",
    "                            text_content = f.read()\n",
    "                        doc_content = f\"\\n\\n{'='*80}\\nDocument: {filename}\\n{'='*80}\\n\\n{text_content}\\n\"\n",
    "                    else:\n",
    "                        doc_content = f\"\\n[ERROR] Unsupported file type: {file_ext}\\n\"\n",
    "                    \n",
    "                    all_text.append(doc_content)\n",
    "                    print(f\"  [OK] {filename} - extracted successfully\")\n",
    "                finally:\n",
    "                    os.unlink(tmp_path)\n",
    "            \n",
    "            # Combine all extracted text\n",
    "            extracted_text = \"\\n\\n\".join(all_text)\n",
    "            \n",
    "            print(f\"\\n[SUCCESS] Successfully extracted text from {len(files)} documents\")\n",
    "            print(f\"Total characters: {len(extracted_text):,}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "process_button.on_click(on_process_button_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(HTML(\"<h3>Upload and Extract Documents</h3>\"))\n",
    "display(HTML(\"<p>Supported formats: PDF, JSON, JSONL, TXT</p>\"))\n",
    "display(upload_widget)\n",
    "display(process_button)\n",
    "display(output)\n",
    "\n",
    "print(\"Use the widget above to select files (PDF, JSON, JSONL, or TXT) and extract their content\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ask Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell prepares the system - no action needed, just run it\n",
    "if extracted_text is None:\n",
    "    print(\"[WARNING] No documents uploaded yet\")\n",
    "    print(\"You can either:\")\n",
    "    print(\"  1. Upload documents in section 4\")\n",
    "    print(\"  2. OR skip to section 7 to use Vespa only\")\n",
    "else:\n",
    "    print(\"[SUCCESS] Documents ready!\")\n",
    "    print(f\"Documents loaded: {', '.join(document_names)}\")\n",
    "    print(f\"Total content: {len(extracted_text):,} characters\")\n",
    "    print(\"\\n[NEXT STEP] Run section 7 (cell 15) to initialize chat interface\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Knowledge Graph & Chat Interface\n",
    "\n",
    "Run this section to build the Knowledge Graph and initialize the chat system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is now handled in section 7 - skip to section 7\n",
    "print(\"Skip to section 7 to initialize the chat interface with Knowledge Graph.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Chat Interface with Knowledge Graph\n",
    "\n",
    "Run this cell to build the Knowledge Graph and prepare the chat system. This only needs to be run once after uploading documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Knowledge Graph (if documents available)\n",
    "use_kg = True\n",
    "kg_retriever = None\n",
    "\n",
    "if extracted_text and use_kg:\n",
    "    try:\n",
    "        from kg_retriever import KGRetriever\n",
    "        \n",
    "        print(\"Building Knowledge Graph for enhanced responses...\")\n",
    "        \n",
    "        # Prepare documents for KG\n",
    "        documents_for_kg = []\n",
    "        for name in document_names:\n",
    "            documents_for_kg.append({\n",
    "                'name': name,\n",
    "                'content': extracted_text\n",
    "            })\n",
    "        \n",
    "        # Build KG\n",
    "        kg_retriever = KGRetriever()\n",
    "        kg_retriever.build_knowledge_graph(documents_for_kg)\n",
    "        \n",
    "        stats = kg_retriever.get_statistics()\n",
    "        print(f\" Knowledge Graph built: {stats['entity_count']} entities, {stats['relationship_count']} relationships\\n\")\n",
    "    except ImportError:\n",
    "        print(\" Knowledge Graph module not found. Using standard chat mode.\\n\")\n",
    "        use_kg = False\n",
    "    except Exception as e:\n",
    "        print(f\" Could not build Knowledge Graph: {str(e)}\")\n",
    "        print(\"Using standard chat mode.\\n\")\n",
    "        use_kg = False\n",
    "elif not extracted_text:\n",
    "    print(\"No documents uploaded - will use Vespa for context if enabled.\\n\")\n",
    "    use_kg = False\n",
    "\n",
    "# Initialize Vespa Vector Store (works with or without documents)\n",
    "vespa_wrapper = None\n",
    "\n",
    "if enable_vespa_toggle.value:\n",
    "    try:\n",
    "        from vespa_search import create_vespa_wrapper\n",
    "        \n",
    "        print(\"Connecting to Vespa Vector Store...\")\n",
    "        \n",
    "        # Get auth parameters if provided\n",
    "        gssso_token = vespa_gssso_input.value if vespa_gssso_input.value else None\n",
    "        api_key = vespa_api_key_input.value if vespa_api_key_input.value else None\n",
    "        \n",
    "        vespa_wrapper = create_vespa_wrapper(\n",
    "            schema_id=vespa_schema_input.value,\n",
    "            env=vespa_env_selector.value,\n",
    "            gssso_token=gssso_token,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        \n",
    "        if vespa_wrapper and vespa_wrapper.is_available():\n",
    "            # Test connection\n",
    "            test_result = vespa_wrapper.test_connection()\n",
    "            \n",
    "            if test_result.get('success'):\n",
    "                print(f\" Vespa connected: {vespa_schema_input.value} ({vespa_env_selector.value})\")\n",
    "                if gssso_token or api_key:\n",
    "                    print(f\" Auth: Using {'GSSO token' if gssso_token else ''} {'+ API key' if api_key else ''}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\" Vespa connection test failed:\")\n",
    "                print(f\" Error: {test_result.get('error')}\")\n",
    "                if test_result.get('suggestion'):\n",
    "                    print(f\" Suggestion: {test_result['suggestion']}\")\n",
    "                print()\n",
    "                vespa_wrapper = None\n",
    "        else:\n",
    "            print(\" Vespa connection failed - will use documents only\\n\")\n",
    "            vespa_wrapper = None\n",
    "    except Exception as e:\n",
    "        print(f\" Could not connect to Vespa: {str(e)}\")\n",
    "        print(\" Will use documents only\\n\")\n",
    "        vespa_wrapper = None\n",
    "\n",
    "# Initialize ReAct Agent (works with or without documents if Vespa available)\n",
    "agent_orchestrator = None\n",
    "query_router = None\n",
    "agent_state = None\n",
    "\n",
    "# Create minimal KG for agent even without documents\n",
    "if enable_agent_toggle.value and not kg_retriever:\n",
    "    from kg_retriever import KGRetriever\n",
    "    kg_retriever = KGRetriever()\n",
    "    print(\"Created minimal KG for agent (will use Vespa for context)\\n\")\n",
    "\n",
    "if enable_agent_toggle.value and kg_retriever:\n",
    "    try:\n",
    "        from query_router import QueryRouter\n",
    "        from react_agent import AgentOrchestrator\n",
    "        from agent_state import AgentState\n",
    "        \n",
    "        print(\"Initializing ReAct Agent...\")\n",
    "        \n",
    "        # Create query router\n",
    "        query_router = QueryRouter(complexity_threshold=agent_complexity_threshold)\n",
    "        \n",
    "        # Create agent orchestrator\n",
    "        agent_orchestrator = AgentOrchestrator(\n",
    "            app_id=app_id,\n",
    "            env=env,\n",
    "            model_name=model_name,\n",
    "            temperature=agent_temperature\n",
    "        )\n",
    "        agent_orchestrator.initialize(\n",
    "            kg_retriever=kg_retriever,\n",
    "            original_documents=extracted_text if extracted_text else \"\",\n",
    "            vespa_wrapper=vespa_wrapper\n",
    "        )\n",
    "        \n",
    "        # Create agent state\n",
    "        agent_state = AgentState()\n",
    "        \n",
    "        tool_count = 10 if vespa_wrapper else 9\n",
    "        print(f\" ReAct Agent initialized successfully!\")\n",
    "        print(f\" - Max iterations: {agent_max_iterations}\")\n",
    "        print(f\" - Complexity threshold: {agent_complexity_threshold}/100\")\n",
    "        print(f\" - {tool_count} specialized tools available\\n\")\n",
    "    except ImportError as e:\n",
    "        print(f\" Agent modules not found: {str(e)}\")\n",
    "        print(\" Using standard chat mode.\\n\")\n",
    "        enable_agent_toggle.value = False\n",
    "    except Exception as e:\n",
    "        print(f\" Could not initialize agent: {str(e)}\")\n",
    "        print(\" Using standard chat mode.\\n\")\n",
    "        enable_agent_toggle.value = False\n",
    "\n",
    "# Chat history\n",
    "chat_history = []\n",
    "\n",
    "# Helper function to extract response\n",
    "def _extract_response(response):\n",
    "    \"\"\"Extract actual content from LLM response.\"\"\"\n",
    "    if hasattr(response, 'content'):\n",
    "        actual_response = response.content\n",
    "    elif isinstance(response, dict):\n",
    "        if 'Response' in response and 'content' in response['Response']:\n",
    "            actual_response = response['Response']['content']\n",
    "        elif 'content' in response:\n",
    "            actual_response = response['content']\n",
    "        else:\n",
    "            actual_response = str(response)\n",
    "    else:\n",
    "        actual_response = str(response)\n",
    "    \n",
    "    # Clean up the response\n",
    "    if actual_response:\n",
    "        actual_response = actual_response.replace('\\\\n\\\\n', '\\n\\n')\n",
    "        actual_response = actual_response.replace('\\\\n', '\\n')\n",
    "        actual_response = actual_response.strip()\n",
    "    \n",
    "    return actual_response\n",
    "\n",
    "def chat_with_documents(question: str) -> str:\n",
    "    \"\"\"Send a question to the LLM and get a response with chat history.\"\"\"\n",
    "    \n",
    "    # Determine if we should use agent\n",
    "    use_agent_for_query = False\n",
    "    routing_info = None\n",
    "    \n",
    "    if enable_agent_toggle.value and agent_orchestrator and query_router:\n",
    "        use_agent_for_query, routing_info = query_router.should_use_agent(question)\n",
    "        \n",
    "    # Route to agent or simple flow\n",
    "    if use_agent_for_query:\n",
    "        # Use ReAct Agent\n",
    "        print(f\"  [Agent Mode] Complexity: {routing_info['complexity_score']}/100\")\n",
    "        \n",
    "        agent_result = agent_orchestrator.query(\n",
    "            query=question,\n",
    "            include_trace=show_reasoning_toggle.value,\n",
    "            state=agent_state\n",
    "        )\n",
    "        \n",
    "        actual_response = agent_result.get('response', 'No response generated')\n",
    "        \n",
    "        # Add routing info if showing trace\n",
    "        if show_reasoning_toggle.value and agent_result.get('trace'):\n",
    "            trace_info = f\"\\n\\n--- Agent Reasoning ---\\n\"\n",
    "            trace_info += f\"Query Type: {routing_info['query_type']}\\n\"\n",
    "            trace_info += f\"Complexity Score: {routing_info['complexity_score']}/100\\n\"\n",
    "            trace_info += f\"Iterations: {agent_result.get('iterations', 'N/A')}\\n\"\n",
    "            trace_info += f\"Routing Reason: {routing_info['routing_reason']}\\n\"\n",
    "            actual_response += trace_info\n",
    "            \n",
    "    # Build enhanced prompt if KG is available (Simple Mode)\n",
    "    elif use_kg and kg_retriever and extracted_text:\n",
    "        print(f\"  [Simple Mode] Using Knowledge Graph retrieval\")\n",
    "        full_prompt = kg_retriever.build_contextual_prompt(question, extracted_text)\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = llm.invoke(full_prompt)\n",
    "        actual_response = _extract_response(response)\n",
    "    elif vespa_wrapper and not extracted_text:\n",
    "        # Use Vespa when no documents uploaded\n",
    "        print(f\"  [Vespa Mode] Searching vector database\")\n",
    "        \n",
    "        vespa_result = vespa_wrapper.search(question, top_k=10)\n",
    "        vespa_context = vespa_wrapper.format_results_for_llm(vespa_result)\n",
    "        \n",
    "        full_prompt = f\"\"\"You are an assistant with access to a vector database.\n",
    "\n",
    "{vespa_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Use the search results above to answer the question\n",
    "2. Cite specific results when providing answers\n",
    "3. If results don't contain relevant information, state that clearly\n",
    "4. Format your response professionally\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = llm.invoke(full_prompt)\n",
    "        actual_response = _extract_response(response)\n",
    "    else:\n",
    "        # Use standard prompt (no KG, no agent, no Vespa)\n",
    "        print(f\"  [Standard Mode] Direct LLM query\")\n",
    "        full_prompt = f\"\"\"You are a cybersecurity and risk analysis assistant. Your role is to help users understand security controls, compliance requirements, risk assessments, and related governance documentation.\n",
    "\n",
    "The documents may contain:\n",
    "- Security controls and compliance frameworks\n",
    "- Risk assessment data and audit findings\n",
    "- Policy documents and governance standards\n",
    "- Tables with control mappings, risk metrics, or compliance data\n",
    "- Structured JSON/JSONL with control definitions, asset types, or security configurations\n",
    "- Regular text describing security procedures and requirements\n",
    "\n",
    "Document Content:\n",
    "{extracted_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Provide accurate, detailed answers based ONLY on the information in the provided documents\n",
    "2. For security controls: Always include control IDs, names, and descriptions when available\n",
    "3. For risk-related queries: Highlight severity, impact, likelihood, and mitigation measures\n",
    "4. For compliance questions: Reference specific requirements, standards, and responsible parties\n",
    "5. Format your response professionally with bullet points and clear organization\n",
    "6. Always cite your sources precisely (e.g., \"Table 2 on Page 5\" or \"JSON Object 3, control_id: 3997\")\n",
    "7. If information is missing, explicitly state what is available and what is not\n",
    "8. Do not include raw JSON dumps - present information in a readable format\n",
    "9. For questions about multiple controls or risks, organize your response systematically\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = llm.invoke(full_prompt)\n",
    "        actual_response = _extract_response(response)\n",
    "    \n",
    "    # Store in chat history\n",
    "    chat_history.append({\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"question\": question,\n",
    "        \"response\": actual_response\n",
    "    })\n",
    "    \n",
    "    return actual_response\n",
    "\n",
    "print(\"[SUCCESS] Interactive chat interface ready!\")\n",
    "if extracted_text:\n",
    "    print(f\"Documents loaded: {', '.join(document_names)}\")\n",
    "if use_kg and kg_retriever and extracted_text:\n",
    "    print(\" Knowledge Graph enhancement: ENABLED\")\n",
    "if vespa_wrapper:\n",
    "    print(f\" Vespa Vector Store: CONNECTED ({vespa_schema_input.value})\")\n",
    "if agent_orchestrator:\n",
    "    tool_count = 10 if vespa_wrapper else 9\n",
    "    print(f\" ReAct Agent: ENABLED ({tool_count} tools available)\")\n",
    "    print(f\" - Complexity threshold: {agent_complexity_threshold}/100\")\n",
    "    print(f\" - Show reasoning: {'Yes' if show_reasoning_toggle.value else 'No'}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if extracted_text:\n",
    "    print(\"You can now chat with your documents!\")\n",
    "elif vespa_wrapper:\n",
    "    print(\"You can now ask questions! (Using Vespa vector database)\")\n",
    "else:\n",
    "    print(\"Upload documents or connect to Vespa to start chatting\")\n",
    "print(\"Type your questions and press Enter.\")\n",
    "print(\"Type 'exit()' to stop chatting.\")\n",
    "print(\"Type 'history' to view chat history.\")\n",
    "if agent_orchestrator:\n",
    "    print(\"\\nAgent will automatically activate for complex queries!\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Continuous Chat\n",
    "\n",
    "Run this cell to start chatting with your documents. You can ask unlimited questions until you type 'exit()'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous chat loop - Run this cell to start chatting\n",
    "if 'chat_with_documents' not in globals():\n",
    "    print(\"[WARNING] Please run section 7 first to initialize the chat interface\")\n",
    "elif extracted_text is None and not vespa_wrapper:\n",
    "    print(\"[WARNING] Please extract text from documents (section 4) OR connect to Vespa (section 7)\")\n",
    "else:\n",
    "    print(\" Chat started! Ask your questions below.\\n\")\n",
    "    \n",
    "    # Continuous chat loop\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            question = input(\"\\n You: \").strip()\n",
    "            \n",
    "            # Check for exit command\n",
    "            if question.lower() in ['exit()', 'exit', 'quit', 'quit()']:\n",
    "                print(\"\\n Exiting chat. Thank you!\")\n",
    "                print(f\"Total questions asked: {len(chat_history)}\")\n",
    "                break\n",
    "            \n",
    "            # Check for history command\n",
    "            if question.lower() == 'history':\n",
    "                if chat_history:\n",
    "                    print(\"\\n Chat History:\")\n",
    "                    print(\"=\"*80)\n",
    "                    for i, entry in enumerate(chat_history, 1):\n",
    "                        print(f\"\\n[{i}] {entry['timestamp']}\")\n",
    "                        print(f\"Q: {entry['question']}\")\n",
    "                        print(f\"A: {entry['response'][:200]}...\" if len(entry['response']) > 200 else f\"A: {entry['response']}\")\n",
    "                        print(\"-\"*80)\n",
    "                else:\n",
    "                    print(\"\\n No chat history yet.\")\n",
    "                continue\n",
    "            \n",
    "            # Skip empty questions\n",
    "            if not question:\n",
    "                print(\" Please enter a question.\")\n",
    "                continue\n",
    "            \n",
    "            # Get response\n",
    "            print(\"\\n Thinking...\")\n",
    "            response = chat_with_documents(question)\n",
    "            \n",
    "            # Display response\n",
    "            print(\"\\n Assistant:\")\n",
    "            print(\"=\"*80)\n",
    "            print(response)\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n Chat interrupted. Type 'exit()' to quit properly.\")\n",
    "            break\n",
    "        except EOFError:\n",
    "            print(\"\\n\\n Exiting chat. Thank you!\")\n",
    "            print(f\"Total questions asked: {len(chat_history)}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Error: {str(e)}\")\n",
    "            print(\"Please try again or type 'exit()' to quit.\")\n",
    "    \n",
    "    print(f\"\\n Session Summary:\")\n",
    "    print(f\"  - Questions asked: {len(chat_history)}\")\n",
    "    print(f\"  - Documents processed: {len(document_names)}\")\n",
    "    if use_kg and kg_retriever:\n",
    "        stats = kg_retriever.get_statistics()\n",
    "        print(f\"  - Entities in KG: {stats['entity_count']}\")\n",
    "        print(f\"  - Relationships in KG: {stats['relationship_count']}\")\n",
    "    if agent_state:\n",
    "        agent_stats = agent_state.get_statistics()\n",
    "        print(f\"  - Agent tool calls: {agent_stats['tool_calls_made']}\")\n",
    "        print(f\"  - Entities discovered: {agent_stats['entities_discovered']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Full Chat History (Optional)\n",
    "\n",
    "Run this cell anytime to view the complete chat history in a formatted table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full chat history as a formatted table\n",
    "if 'chat_history' not in globals():\n",
    "    print(\" Please run section 7 first to initialize the chat interface\")\n",
    "elif not chat_history:\n",
    "    print(\" No chat history yet. Start asking questions in section 8!\")\n",
    "else:\n",
    "    print(f\" Chat History ({len(chat_history)} questions)\\n\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for i, entry in enumerate(chat_history, 1):\n",
    "        print(f\"\\n[Question {i}] {entry['timestamp']}\")\n",
    "        print(f\"{'─'*100}\")\n",
    "        print(f\" Question: {entry['question']}\")\n",
    "        print(f\"\\n Answer:\")\n",
    "        print(entry['response'])\n",
    "        print(\"=\"*100)\n",
    "    \n",
    "    # Also display as DataFrame for easy export\n",
    "    print(\"\\n Exportable Table View:\\n\")\n",
    "    df_history = pd.DataFrame(chat_history)\n",
    "    display(df_history)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\n Statistics:\")\n",
    "    print(f\"  - Total questions: {len(chat_history)}\")\n",
    "    print(f\"  - Average response length: {sum(len(e['response']) for e in chat_history) / len(chat_history):.0f} characters\")\n",
    "    \n",
    "    if use_kg and kg_retriever:\n",
    "        stats = kg_retriever.get_statistics()\n",
    "        print(f\"  - Knowledge Graph entities: {stats['entity_count']}\")\n",
    "        print(f\"  - Knowledge Graph relationships: {stats['relationship_count']}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
