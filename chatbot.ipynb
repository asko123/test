{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Chat Bot with Knowledge Graph\n",
    "\n",
    "This notebook provides an interactive chat interface for querying multiple documents (PDF, JSON, JSONL, TXT) with Knowledge Graph enhancement for better responses.\n",
    "\n",
    "## Quick Start Workflow\n",
    "\n",
    "1. **Run cells 1-4**: Install packages, import libraries, configure settings\n",
    "2. **Run cell 9**: Upload your documents and extract text\n",
    "3. **Run cell 15**: Initialize Knowledge Graph and chat interface\n",
    "4. **Run cell 17**: Start continuous chat - ask unlimited questions until you type `exit()`\n",
    "5. **Run cell 19** (optional): View full chat history\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Multi-format support**: PDF, JSON, JSONL, and TXT files\n",
    "- **Knowledge Graph**: Automatically extracts entities (controls, risks, assets, etc.) and relationships\n",
    "- **Continuous chat**: Ask multiple questions in one session\n",
    "- **Chat history**: View all questions and answers\n",
    "- **Enhanced responses**: Better context through entity and relationship tracking\n",
    "\n",
    "## Commands During Chat\n",
    "\n",
    "- Type your question and press Enter\n",
    "- Type `exit()` or `quit()` to end the chat session\n",
    "- Type `history` to view chat history\n",
    "- Press Ctrl+C to interrupt (then type exit() to quit properly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install goldmansachs.awm_genai -U\n",
    "%pip install python-dotenv pandas ipywidgets pdfplumber networkx matplotlib -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goldmansachs.awm_genai import LLM, LLMConfig\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "import pdfplumber\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "app_id = \"trai\"\n",
    "env = \"uat\"\n",
    "\n",
    "# Model Configuration - Choose your model\n",
    "available_models = [\"gemini-2.5-pro\", \"gemini-2.5-flash-lite\"]\n",
    "\n",
    "# Create model selection widget\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=\"gemini-2.5-flash-lite\",\n",
    "    description='Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Display model selector\n",
    "display(HTML(\"<h3>Select Model</h3>\"))\n",
    "display(HTML(\"<p><b>gemini-2.5-pro:</b> More capable, better for complex questions<br><b>gemini-2.5-flash-lite:</b> Faster responses, good for simple queries</p>\"))\n",
    "display(model_selector)\n",
    "\n",
    "# Store configuration\n",
    "temperature = 0\n",
    "log_level = \"DEBUG\"\n",
    "\n",
    "print(f\"\\nApp ID: {app_id}\")\n",
    "print(f\"Environment: {env}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM with selected model\n",
    "model_name = model_selector.value\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    app_id=app_id,\n",
    "    env=env,\n",
    "    model_name=model_name,\n",
    "    temperature=temperature,\n",
    "    log_level=log_level,\n",
    ")\n",
    "\n",
    "llm = LLM.init(config=llm_config)\n",
    "print(f\"[SUCCESS] LLM initialized successfully with {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Documents\n",
    "\n",
    "Use the file upload widget below to select your PDF documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract PDF content with tables and JSON\n",
    "def extract_pdf_content(file_path: str, filename: str) -> str:\n",
    "    \"\"\"Extract text, tables, and JSON from PDF.\"\"\"\n",
    "    content_parts = [f\"\\n\\n{'='*80}\\nDocument: {filename}\\n{'='*80}\\n\"]\n",
    "    \n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            content_parts.append(f\"\\n[Page {page_num}]\\n\")\n",
    "            \n",
    "            # Extract tables on this page\n",
    "            tables = page.extract_tables()\n",
    "            \n",
    "            # Get bounding boxes of tables to exclude from text\n",
    "            table_bboxes = []\n",
    "            if tables:\n",
    "                for table in page.find_tables():\n",
    "                    table_bboxes.append(table.bbox)\n",
    "            \n",
    "            # Extract text excluding table areas\n",
    "            if table_bboxes:\n",
    "                text = page.filter(lambda obj: not any(\n",
    "                    obj.get('x0', 0) >= bbox[0] and obj.get('x1', 0) <= bbox[2] and\n",
    "                    obj.get('top', 0) >= bbox[1] and obj.get('bottom', 0) <= bbox[3]\n",
    "                    for bbox in table_bboxes\n",
    "                )).extract_text()\n",
    "            else:\n",
    "                text = page.extract_text()\n",
    "            \n",
    "            # Check for JSON/JSONL content in text\n",
    "            if text and text.strip():\n",
    "                json_objects = extract_json_content(text)\n",
    "                \n",
    "                if json_objects:\n",
    "                    # Add regular text (non-JSON parts)\n",
    "                    non_json_text = remove_json_from_text(text)\n",
    "                    if non_json_text.strip():\n",
    "                        content_parts.append(f\"{non_json_text}\\n\")\n",
    "                    \n",
    "                    # Add formatted JSON objects\n",
    "                    for json_idx, json_obj in enumerate(json_objects, 1):\n",
    "                        formatted_json = format_json_object(json_obj, page_num, json_idx, filename)\n",
    "                        content_parts.append(f\"\\n{formatted_json}\\n\")\n",
    "                else:\n",
    "                    # No JSON, add as regular text\n",
    "                    content_parts.append(f\"{text}\\n\")\n",
    "            \n",
    "            # Add tables with proper formatting\n",
    "            if tables:\n",
    "                for table_idx, table in enumerate(tables, 1):\n",
    "                    if table and len(table) > 0:\n",
    "                        formatted_table = format_table(table, page_num, table_idx, filename)\n",
    "                        content_parts.append(f\"\\n{formatted_table}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "def extract_json_content(text: str) -> list:\n",
    "    \"\"\"Extract JSON or JSONL objects from text.\"\"\"\n",
    "    json_objects = []\n",
    "    \n",
    "    # Try to find JSON objects\n",
    "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
    "    matches = re.finditer(json_pattern, text, re.DOTALL)\n",
    "    \n",
    "    for match in matches:\n",
    "        try:\n",
    "            json_str = match.group(0)\n",
    "            json_obj = json.loads(json_str)\n",
    "            json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Also try line-by-line for JSONL format\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('{') and line.endswith('}'):\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                if json_obj not in json_objects:\n",
    "                    json_objects.append(json_obj)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return json_objects\n",
    "\n",
    "def remove_json_from_text(text: str) -> str:\n",
    "    \"\"\"Remove JSON objects from text to get only regular text.\"\"\"\n",
    "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
    "    cleaned_text = re.sub(json_pattern, '', text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "\n",
    "def format_json_object(json_obj: dict, page_num: int, json_idx: int, filename: str) -> str:\n",
    "    \"\"\"Format JSON object for LLM understanding.\"\"\"\n",
    "    json_parts = []\n",
    "    json_parts.append(f\"--- JSON OBJECT {json_idx} (Document: {filename}, Page {page_num}) ---\")\n",
    "    \n",
    "    # Add formatted key-value pairs\n",
    "    json_parts.append(\"\\nStructured Data Fields:\")\n",
    "    for key, value in json_obj.items():\n",
    "        # Clean up the value\n",
    "        if isinstance(value, str):\n",
    "            value = ' '.join(value.split())\n",
    "        json_parts.append(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Add JSON format\n",
    "    json_parts.append(\"\\nJSON Format:\")\n",
    "    json_parts.append(json.dumps(json_obj, indent=2))\n",
    "    \n",
    "    json_parts.append(f\"--- END JSON OBJECT {json_idx} ---\\n\")\n",
    "    \n",
    "    return \"\\n\".join(json_parts)\n",
    "\n",
    "def format_table(table: list, page_num: int, table_idx: int, filename: str) -> str:\n",
    "    \"\"\"Format table with proper structure.\"\"\"\n",
    "    if not table or len(table) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean table data\n",
    "    cleaned_table = []\n",
    "    for row in table:\n",
    "        cleaned_row = [str(cell).strip() if cell is not None else \"\" for cell in row]\n",
    "        if any(cleaned_row):\n",
    "            cleaned_table.append(cleaned_row)\n",
    "    \n",
    "    if not cleaned_table:\n",
    "        return \"\"\n",
    "    \n",
    "    table_parts = []\n",
    "    table_parts.append(f\"--- TABLE {table_idx} (Document: {filename}, Page {page_num}) ---\")\n",
    "    \n",
    "    # Assume first row is header\n",
    "    headers = cleaned_table[0]\n",
    "    data_rows = cleaned_table[1:]\n",
    "    \n",
    "    # Add headers\n",
    "    table_parts.append(\"\\nColumn Headers:\")\n",
    "    table_parts.append(\" | \".join(headers))\n",
    "    table_parts.append(\"-\" * 80)\n",
    "    \n",
    "    # Add data rows\n",
    "    table_parts.append(\"\\nTable Data:\")\n",
    "    for row in data_rows:\n",
    "        table_parts.append(\" | \".join(row))\n",
    "    \n",
    "    # Add markdown format for better LLM understanding\n",
    "    table_parts.append(\"\\nMarkdown Format:\")\n",
    "    table_parts.append(\"| \" + \" | \".join(headers) + \" |\")\n",
    "    table_parts.append(\"|\" + \"|\".join([\"---\" for _ in headers]) + \"|\")\n",
    "    for row in data_rows:\n",
    "        table_parts.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "    \n",
    "    table_parts.append(f\"--- END TABLE {table_idx} ---\\n\")\n",
    "    \n",
    "    return \"\\n\".join(table_parts)\n",
    "\n",
    "# Helper functions for JSON extraction\n",
    "def extract_from_json_file(file_path: str, filename: str) -> str:\n",
    "    \"\"\"Extract and format JSON file content.\"\"\"\n",
    "    content_parts = [f\"\\n\\n{'='*80}\\nDocument: {filename}\\n{'='*80}\\n\"]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            content_parts.append(\"\\nThis file contains a list of JSON objects:\\n\")\n",
    "            for idx, obj in enumerate(data, 1):\n",
    "                formatted = format_json_object(obj, 0, idx, filename)\n",
    "                content_parts.append(f\"\\n{formatted}\\n\")\n",
    "        elif isinstance(data, dict):\n",
    "            formatted = format_json_object(data, 0, 1, filename)\n",
    "            content_parts.append(f\"\\n{formatted}\\n\")\n",
    "        else:\n",
    "            content_parts.append(f\"\\nJSON Value: {data}\\n\")\n",
    "    except Exception as e:\n",
    "        content_parts.append(f\"\\n[ERROR] Failed to parse JSON: {str(e)}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "def extract_from_jsonl_file(file_path: str, filename: str) -> str:\n",
    "    \"\"\"Extract and format JSONL file content.\"\"\"\n",
    "    content_parts = [f\"\\n\\n{'='*80}\\nDocument: {filename}\\n{'='*80}\\n\"]\n",
    "    content_parts.append(\"\\nThis file contains multiple JSON objects (one per line):\\n\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        formatted = format_json_object(obj, 0, idx, filename)\n",
    "                        content_parts.append(f\"\\n{formatted}\\n\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        content_parts.append(f\"\\n[Line {idx}] Invalid JSON: {line[:100]}...\\n\")\n",
    "    except Exception as e:\n",
    "        content_parts.append(f\"\\n[ERROR] Failed to parse JSONL: {str(e)}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "# Create file upload widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.pdf,.json,.jsonl,.txt',\n",
    "    multiple=True,\n",
    "    description='Select Files'\n",
    ")\n",
    "\n",
    "# Create process button\n",
    "process_button = widgets.Button(\n",
    "    description='Extract Text',\n",
    "    button_style='primary',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Create output widget for status messages\n",
    "output = widgets.Output()\n",
    "\n",
    "# Store extracted text globally\n",
    "extracted_text = None\n",
    "document_names = []\n",
    "\n",
    "def on_process_button_clicked(b):\n",
    "    global extracted_text, document_names\n",
    "    \n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        \n",
    "        if not upload_widget.value:\n",
    "            print(\"[WARNING] Please select PDF files first\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            all_text = []\n",
    "            document_names = []\n",
    "            \n",
    "            # Extract content from uploaded files\n",
    "            files = upload_widget.value\n",
    "            print(f\"Processing {len(files)} files...\\n\")\n",
    "            \n",
    "            temp_files = []\n",
    "            document_names = []\n",
    "            \n",
    "            # Save uploaded files temporarily\n",
    "            for file_info in files:\n",
    "                filename = file_info['name']\n",
    "                content = file_info['content']\n",
    "                document_names.append(filename)\n",
    "                \n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "                    tmp_file.write(content)\n",
    "                    temp_files.append((tmp_file.name, filename))\n",
    "            \n",
    "            # Extract content based on file type\n",
    "            for tmp_path, filename in temp_files:\n",
    "                try:\n",
    "                    file_ext = filename.split('.')[-1].lower()\n",
    "                    \n",
    "                    if file_ext == 'pdf':\n",
    "                        doc_content = extract_pdf_content(tmp_path, filename)\n",
    "                    elif file_ext == 'json':\n",
    "                        doc_content = extract_from_json_file(tmp_path, filename)\n",
    "                    elif file_ext == 'jsonl':\n",
    "                        doc_content = extract_from_jsonl_file(tmp_path, filename)\n",
    "                    elif file_ext == 'txt':\n",
    "                        # Extract from text file\n",
    "                        with open(tmp_path, 'r', encoding='utf-8') as f:\n",
    "                            text_content = f.read()\n",
    "                        doc_content = f\"\\n\\n{'='*80}\\nDocument: {filename}\\n{'='*80}\\n\\n{text_content}\\n\"\n",
    "                    else:\n",
    "                        doc_content = f\"\\n[ERROR] Unsupported file type: {file_ext}\\n\"\n",
    "                    \n",
    "                    all_text.append(doc_content)\n",
    "                    print(f\"  [OK] {filename} - extracted successfully\")\n",
    "                finally:\n",
    "                    os.unlink(tmp_path)\n",
    "            \n",
    "            # Combine all extracted text\n",
    "            extracted_text = \"\\n\\n\".join(all_text)\n",
    "            \n",
    "            print(f\"\\n[SUCCESS] Successfully extracted text from {len(files)} documents\")\n",
    "            print(f\"Total characters: {len(extracted_text):,}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "process_button.on_click(on_process_button_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(HTML(\"<h3>Upload and Extract Documents</h3>\"))\n",
    "display(HTML(\"<p>Supported formats: PDF, JSON, JSONL, TXT</p>\"))\n",
    "display(upload_widget)\n",
    "display(process_button)\n",
    "display(output)\n",
    "\n",
    "print(\"Use the widget above to select files (PDF, JSON, JSONL, or TXT) and extract their content\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ask Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell prepares the system - no action needed, just run it\n",
    "if extracted_text is None:\n",
    "    print(\"[WARNING] Please extract text from documents first (see section 4)\")\n",
    "else:\n",
    "    print(\"[SUCCESS] Documents ready!\")\n",
    "    print(f\"Documents loaded: {', '.join(document_names)}\")\n",
    "    print(f\"Total content: {len(extracted_text):,} characters\")\n",
    "    print(\"\\nProceed to section 7 to initialize the chat interface.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Knowledge Graph & Chat Interface\n",
    "\n",
    "Run this section to build the Knowledge Graph and initialize the chat system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is now handled in section 7 - skip to section 7\n",
    "print(\"⏭️  Skip to section 7 to initialize the chat interface with Knowledge Graph.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Chat Interface with Knowledge Graph\n",
    "\n",
    "Run this cell to build the Knowledge Graph and prepare the chat system. This only needs to be run once after uploading documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is ready\n",
    "if extracted_text is None:\n",
    "    print(\"[WARNING] Please extract text from PDFs first (see section 4)\")\n",
    "else:\n",
    "    # Initialize Knowledge Graph (optional but recommended for better responses)\n",
    "    use_kg = True\n",
    "    kg_retriever = None\n",
    "    \n",
    "    if use_kg:\n",
    "        try:\n",
    "            from kg_retriever import KGRetriever\n",
    "            \n",
    "            print(\"Building Knowledge Graph for enhanced responses...\")\n",
    "            \n",
    "            # Prepare documents for KG\n",
    "            documents_for_kg = []\n",
    "            for name in document_names:\n",
    "                documents_for_kg.append({\n",
    "                    'name': name,\n",
    "                    'content': extracted_text\n",
    "                })\n",
    "            \n",
    "            # Build KG\n",
    "            kg_retriever = KGRetriever()\n",
    "            kg_retriever.build_knowledge_graph(documents_for_kg)\n",
    "            \n",
    "            stats = kg_retriever.get_statistics()\n",
    "            print(f\" Knowledge Graph built: {stats['entity_count']} entities, {stats['relationship_count']} relationships\\n\")\n",
    "        except ImportError:\n",
    "            print(\" Knowledge Graph module not found. Using standard chat mode.\\n\")\n",
    "            use_kg = False\n",
    "        except Exception as e:\n",
    "            print(f\" Could not build Knowledge Graph: {str(e)}\")\n",
    "            print(\"Using standard chat mode.\\n\")\n",
    "            use_kg = False\n",
    "    \n",
    "    # Chat history\n",
    "    chat_history = []\n",
    "    \n",
    "    def chat_with_documents(question: str) -> str:\n",
    "        \"\"\"Send a question to the LLM and get a response with chat history.\"\"\"\n",
    "        # Build enhanced prompt if KG is available\n",
    "        if use_kg and kg_retriever:\n",
    "            full_prompt = kg_retriever.build_contextual_prompt(question, extracted_text)\n",
    "        else:\n",
    "            # Use standard prompt\n",
    "            full_prompt = f\"\"\"You are a cybersecurity and risk analysis assistant. Your role is to help users understand security controls, compliance requirements, risk assessments, and related governance documentation.\n",
    "\n",
    "The documents may contain:\n",
    "- Security controls and compliance frameworks\n",
    "- Risk assessment data and audit findings\n",
    "- Policy documents and governance standards\n",
    "- Tables with control mappings, risk metrics, or compliance data\n",
    "- Structured JSON/JSONL with control definitions, asset types, or security configurations\n",
    "- Regular text describing security procedures and requirements\n",
    "\n",
    "Document Content:\n",
    "{extracted_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Provide accurate, detailed answers based ONLY on the information in the provided documents\n",
    "2. For security controls: Always include control IDs, names, and descriptions when available\n",
    "3. For risk-related queries: Highlight severity, impact, likelihood, and mitigation measures\n",
    "4. For compliance questions: Reference specific requirements, standards, and responsible parties\n",
    "5. Format your response professionally with bullet points and clear organization\n",
    "6. Always cite your sources precisely (e.g., \"Table 2 on Page 5\" or \"JSON Object 3, control_id: 3997\")\n",
    "7. If information is missing, explicitly state what is available and what is not\n",
    "8. Do not include raw JSON dumps - present information in a readable format\n",
    "9. For questions about multiple controls or risks, organize your response systematically\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Get response\n",
    "        response = llm.invoke(full_prompt)\n",
    "        \n",
    "        # Extract actual content from response\n",
    "        actual_response = None\n",
    "        \n",
    "        # Try different response formats\n",
    "        if hasattr(response, 'content'):\n",
    "            actual_response = response.content\n",
    "        elif isinstance(response, dict):\n",
    "            if 'Response' in response and 'content' in response['Response']:\n",
    "                actual_response = response['Response']['content']\n",
    "            elif 'content' in response:\n",
    "                actual_response = response['content']\n",
    "            else:\n",
    "                actual_response = str(response)\n",
    "        else:\n",
    "            actual_response = str(response)\n",
    "        \n",
    "        # Clean up the response\n",
    "        if actual_response:\n",
    "            actual_response = actual_response.replace('\\\\n\\\\n', '\\n\\n')\n",
    "            actual_response = actual_response.replace('\\\\n', '\\n')\n",
    "            actual_response = actual_response.strip()\n",
    "        \n",
    "        # Store in chat history\n",
    "        chat_history.append({\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"question\": question,\n",
    "            \"response\": actual_response\n",
    "        })\n",
    "        \n",
    "        return actual_response\n",
    "    \n",
    "    print(\"[SUCCESS] Interactive chat interface ready!\")\n",
    "    print(f\"Documents loaded: {', '.join(document_names)}\")\n",
    "    if use_kg and kg_retriever:\n",
    "        print(\" Knowledge Graph enhancement: ENABLED\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"You can now chat with your documents!\")\n",
    "    print(\"Type your questions and press Enter.\")\n",
    "    print(\"Type 'exit()' to stop chatting.\")\n",
    "    print(\"Type 'history' to view chat history.\")\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Continuous Chat\n",
    "\n",
    "Run this cell to start chatting with your documents. You can ask unlimited questions until you type 'exit()'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous chat loop - Run this cell to start chatting\n",
    "if 'chat_with_documents' not in globals():\n",
    "    print(\"[WARNING] Please run section 7 first to initialize the chat interface\")\n",
    "elif extracted_text is None:\n",
    "    print(\"[WARNING] Please extract text from documents first (see section 4)\")\n",
    "else:\n",
    "    print(\" Chat started! Ask your questions below.\\n\")\n",
    "    \n",
    "    # Continuous chat loop\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            question = input(\"\\n You: \").strip()\n",
    "            \n",
    "            # Check for exit command\n",
    "            if question.lower() in ['exit()', 'exit', 'quit', 'quit()']:\n",
    "                print(\"\\n Exiting chat. Thank you!\")\n",
    "                print(f\"Total questions asked: {len(chat_history)}\")\n",
    "                break\n",
    "            \n",
    "            # Check for history command\n",
    "            if question.lower() == 'history':\n",
    "                if chat_history:\n",
    "                    print(\"\\n Chat History:\")\n",
    "                    print(\"=\"*80)\n",
    "                    for i, entry in enumerate(chat_history, 1):\n",
    "                        print(f\"\\n[{i}] {entry['timestamp']}\")\n",
    "                        print(f\"Q: {entry['question']}\")\n",
    "                        print(f\"A: {entry['response'][:200]}...\" if len(entry['response']) > 200 else f\"A: {entry['response']}\")\n",
    "                        print(\"-\"*80)\n",
    "                else:\n",
    "                    print(\"\\n No chat history yet.\")\n",
    "                continue\n",
    "            \n",
    "            # Skip empty questions\n",
    "            if not question:\n",
    "                print(\" Please enter a question.\")\n",
    "                continue\n",
    "            \n",
    "            # Get response\n",
    "            print(\"\\n Thinking...\")\n",
    "            response = chat_with_documents(question)\n",
    "            \n",
    "            # Display response\n",
    "            print(\"\\n Assistant:\")\n",
    "            print(\"=\"*80)\n",
    "            print(response)\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n Chat interrupted. Type 'exit()' to quit properly.\")\n",
    "            break\n",
    "        except EOFError:\n",
    "            print(\"\\n\\n Exiting chat. Thank you!\")\n",
    "            print(f\"Total questions asked: {len(chat_history)}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Error: {str(e)}\")\n",
    "            print(\"Please try again or type 'exit()' to quit.\")\n",
    "    \n",
    "    print(f\"\\n Session Summary:\")\n",
    "    print(f\"  - Questions asked: {len(chat_history)}\")\n",
    "    print(f\"  - Documents processed: {len(document_names)}\")\n",
    "    if use_kg and kg_retriever:\n",
    "        stats = kg_retriever.get_statistics()\n",
    "        print(f\"  - Entities in KG: {stats['entity_count']}\")\n",
    "        print(f\"  - Relationships in KG: {stats['relationship_count']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Full Chat History (Optional)\n",
    "\n",
    "Run this cell anytime to view the complete chat history in a formatted table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full chat history as a formatted table\n",
    "if 'chat_history' not in globals():\n",
    "    print(\" Please run section 7 first to initialize the chat interface\")\n",
    "elif not chat_history:\n",
    "    print(\" No chat history yet. Start asking questions in section 8!\")\n",
    "else:\n",
    "    print(f\" Chat History ({len(chat_history)} questions)\\n\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for i, entry in enumerate(chat_history, 1):\n",
    "        print(f\"\\n[Question {i}] {entry['timestamp']}\")\n",
    "        print(f\"{'─'*100}\")\n",
    "        print(f\" Question: {entry['question']}\")\n",
    "        print(f\"\\n Answer:\")\n",
    "        print(entry['response'])\n",
    "        print(\"=\"*100)\n",
    "    \n",
    "    # Also display as DataFrame for easy export\n",
    "    print(\"\\n Exportable Table View:\\n\")\n",
    "    df_history = pd.DataFrame(chat_history)\n",
    "    display(df_history)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\n Statistics:\")\n",
    "    print(f\"  - Total questions: {len(chat_history)}\")\n",
    "    print(f\"  - Average response length: {sum(len(e['response']) for e in chat_history) / len(chat_history):.0f} characters\")\n",
    "    \n",
    "    if use_kg and kg_retriever:\n",
    "        stats = kg_retriever.get_statistics()\n",
    "        print(f\"  - Knowledge Graph entities: {stats['entity_count']}\")\n",
    "        print(f\"  - Knowledge Graph relationships: {stats['relationship_count']}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}